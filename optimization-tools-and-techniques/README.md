# Optimizing Intel® Computer Vision SDK Applications
This tutorial shows some techniques to get better performance from computer vision applications with model optimizer and inference engine.

### 1. Tuning parameters in Model Obtimizer

### 2. Pick the right model based on application and hardware
Use/train a model with the right performance/accuracy tradeoffs.   Performance differences between models can be bigger than any optimization you can do at the inference app level.

### 3. Use an optimized inference implementation.  
Performance difference between using Inference Engine and a non-optimized framework can be bigger than differences between accelerators. 

### 4. Use the right data type for your target HW and accuracy needs

### 5. Use async

### 5. Don’t infer every frame if not needed



